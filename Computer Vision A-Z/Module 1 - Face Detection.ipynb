{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c781e7b6",
   "metadata": {},
   "source": [
    "### Viola-Jones Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c9e2e6",
   "metadata": {},
   "source": [
    "Developed by Paul Viola and Michael Jones\n",
    "Designed to recognize forward-facing images of faces.\n",
    "Images are in greyscale\n",
    "Proceeds in a grid-search fashion and recognizes facial characteristics (eyes, nose etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a8458",
   "metadata": {},
   "source": [
    "### Haar-Like Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47562191",
   "metadata": {},
   "source": [
    "Based on Alfred Haar\n",
    "Edge features, line features and four-rectangle features\n",
    "Consider a grid of greyscale images. Compute average greyscale intensity and find the corresponding haar-like features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fb7d9",
   "metadata": {},
   "source": [
    "### Integral Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15542d73",
   "metadata": {},
   "source": [
    "Computing sums and averages on a grid can be computationally expensive. The solution to this is the integral image\n",
    "For each particular cell in the grid, compute the sum of all squares to the left and above the cell (including the cell in question). Helpful because Haar-like features are rectangular.\n",
    "Regardless of the size of the rectangle, integral image will only require us to process 4 values regardless of the size of the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63092b1",
   "metadata": {},
   "source": [
    "### Adaptive Boosting (AdaBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e6e00",
   "metadata": {},
   "source": [
    "Over 180,000+ features per image. Boosting helps as follows:\n",
    "- We generate some linear combination of all our features with specific weights\n",
    "- A weak classifier may be one of the features by itself\n",
    "- A strong classifier may be any linear combination of multiple weak classifier\n",
    "- Overall this is an ensemble method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3810ec8e",
   "metadata": {},
   "source": [
    "### Cascading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772abfc",
   "metadata": {},
   "source": [
    "- Start with a sub-window\n",
    "- If feature is not present in sub-window, reject the sub-window\n",
    "- If feature is in the sub-window, look for our second feature\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9aaea1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a4be7b2bfa19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_capture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# We get the last frame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# We do some colour transformations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mcanvas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# We get the output of our detect function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Video'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanvas\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# We display the outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xFF\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# If we type on the keyboard:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a4be7b2bfa19>\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(gray, frame)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mroi_gray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# We get the region of interest in the black and white image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mroi_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# We get the region of interest in the colored image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0meyes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meye_cascade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroi_gray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# We apply the detectMultiScale method to locate one or several eyes in the image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meyes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# For each detected eye:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroi_color\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mey\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0meh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# We paint a rectangle around the eyes, but inside the referential of the face.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Face Recognition\n",
    "\n",
    "import cv2\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') # We load the cascade for the face.\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml') # We load the cascade for the eyes.\n",
    "\n",
    "def detect(gray, frame): # We create a function that takes as input the image in black and white (gray) and the original image (frame), and that will return the same image with the detector rectangles. \n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5) # We apply the detectMultiScale method from the face cascade to locate one or several faces in the image.\n",
    "    for (x, y, w, h) in faces: # For each detected face:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2) # We paint a rectangle around the face.\n",
    "        roi_gray = gray[y:y+h, x:x+w] # We get the region of interest in the black and white image.\n",
    "        roi_color = frame[y:y+h, x:x+w] # We get the region of interest in the colored image.\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, 1.1, 3) # We apply the detectMultiScale method to locate one or several eyes in the image.\n",
    "        for (ex, ey, ew, eh) in eyes: # For each detected eye:\n",
    "            cv2.rectangle(roi_color,(ex, ey),(ex+ew, ey+eh), (0, 255, 0), 2) # We paint a rectangle around the eyes, but inside the referential of the face.\n",
    "    return frame # We return the image with the detector rectangles.\n",
    "\n",
    "video_capture = cv2.VideoCapture(0) # We turn the webcam on.\n",
    "\n",
    "while True: # We repeat infinitely (until break):\n",
    "    _, frame = video_capture.read() # We get the last frame.\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # We do some colour transformations.\n",
    "    canvas = detect(gray, frame) # We get the output of our detect function.\n",
    "    cv2.imshow('Video', canvas) # We display the outputs.\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): # If we type on the keyboard:\n",
    "        break # We stop the loop.\n",
    "\n",
    "video_capture.release() # We turn the webcam off.\n",
    "cv2.destroyAllWindows() # We destroy all the windows inside which the images were displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b89e32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
