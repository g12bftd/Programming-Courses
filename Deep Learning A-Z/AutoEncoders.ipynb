{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A type of artificial neural network that consists of:\n",
    "- Visible input nodes\n",
    "- Encoding process\n",
    "- Hidden Layers\n",
    "- Decoding process\n",
    "- Visible output nodes\n",
    "\n",
    "This is a self-supervised learning method.\n",
    "\n",
    "The aim of an autoencoder is to leanr a representation (encoding) of a set of a data by ignoring signal \"noise\" and reducing dimensions. \n",
    "\n",
    "E.g. translation of human languages, medical imaging, recommendation systems\n",
    "\n",
    "\n",
    "A note on biases:\n",
    "Our autoencoder may enclode bias terms to re-centre the data.\n",
    "\n",
    "\n",
    "#### Training an autoencoder (movie ratings example):\n",
    "\n",
    "1) Start with an array where row = user and column = movie. Each cell contains the rating 1-5 (0 if no rating) of the user u for movie i\n",
    "\n",
    "2) First user is passed to the network, with input vector containing all ratings.\n",
    "\n",
    "3) Input vector is encoded into a vector z of lower dimensions by a mapping function (e.g. sigmoid or tanh)\n",
    "\n",
    "4) z is decoded into output vector y of the same dimensions as x, aiming to replicate the input vector x\n",
    "\n",
    "5) The reconstruction error d(x, y) is computed. The goal is to minimize it\n",
    "\n",
    "6) Using backpropagation, weights are updated according to how much they are responsible for the error. Learning rate determines how much we update the weights by.\n",
    "\n",
    "7) Repeat steps 1-6 and update weights after each obsercation, or repeat 1-6 but update the weights only after a batch of observations.\n",
    "\n",
    "8) When the whole training set is pased through the ANN, that becomes 1 epoch. Repeat for a set number of epochs.\n",
    "\n",
    "\n",
    "#### Overcomplete hidden layers:\n",
    "Suppose we had more nodes in the hidden layer than in the input layer. Although autoencoders are typically convenient for feature extraction/dimensionality reduction, nothing prevents us from having more nodes in the hidden layer than in the input layer.\n",
    "\n",
    "PROBLEM:\n",
    "If we have too many nodes in the hidden layer, the autoencoder's backpropagation algorithm can set some node weights to 0 or 1 such that some nodes are unused, and others are used at 100% weight (e.g. weight = 1). \n",
    "\n",
    "\n",
    "#### Sparse autoencoders:\n",
    "It has been observed that when representations are learnt in a way that encourages sparsity, impproved performance is obtained on classification tasks.\n",
    "\n",
    "Sparse autoencoders may include more hidden units than inputs, but only a small number of these hidden units are allowed to be active at once. This sparsity constraint forces the model to respond to the unique statistical features of the input data used for training. \n",
    "\n",
    "Specifically, a sparse autoencoder will have a training criterion that has a sparsity penalty on each layer. This penalty encourage the model to activate (output value close to 1) some specific areas of the network on the basis of the input data, while forcing all other neurons to be inactive (output values close to 0).\n",
    "\n",
    "#### Denoising autoencoders:\n",
    "Denoising autoencoders (DAE) try to achieve a good representation by changing the reconstruction criterion of the model.\n",
    "\n",
    "DAEs take a partially corrupted input and are trained to recover the original undistorted input. In practice, the objective of denoising autoencoders is that of cleaning the corrupted input.\n",
    "\n",
    "- Higher level representations much be relatively stable and robust to the corruption of the input\n",
    "\n",
    "- The model needs to extract features that capture usefule structure in the distribution of the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\\t')\n",
    "training_set = np.array(training_set, dtype = 'int')\n",
    "test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\\t')\n",
    "test_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_users = int(max(max(training_set[:, 0], ), max(test_set[:, 0])))\n",
    "nb_movies = int(max(max(training_set[:, 1], ), max(test_set[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "  new_data = []\n",
    "  for id_users in range(1, nb_users + 1):\n",
    "    id_movies = data[:, 1] [data[:, 0] == id_users]\n",
    "    id_ratings = data[:, 2] [data[:, 0] == id_users]\n",
    "    ratings = np.zeros(nb_movies)\n",
    "    ratings[id_movies - 1] = id_ratings\n",
    "    new_data.append(list(ratings))\n",
    "  return new_data\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating architecture for stacked autoencoder\n",
    "# We will use inheritence via the nn library\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__()\n",
    "        # Now create linear transformations of data\n",
    "        self.fc1 = nn.Linear(in_features = nb_movies, out_features = 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), \n",
    "                          lr = 0.01, weight_decay = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: tensor(0.9210)\n",
      "epoch: 2 loss: tensor(0.9233)\n",
      "epoch: 3 loss: tensor(0.9207)\n",
      "epoch: 4 loss: tensor(0.9229)\n",
      "epoch: 5 loss: tensor(0.9207)\n",
      "epoch: 6 loss: tensor(0.9225)\n",
      "epoch: 7 loss: tensor(0.9203)\n",
      "epoch: 8 loss: tensor(0.9221)\n",
      "epoch: 9 loss: tensor(0.9200)\n",
      "epoch: 10 loss: tensor(0.9214)\n",
      "epoch: 11 loss: tensor(0.9198)\n",
      "epoch: 12 loss: tensor(0.9211)\n",
      "epoch: 13 loss: tensor(0.9193)\n",
      "epoch: 14 loss: tensor(0.9208)\n",
      "epoch: 15 loss: tensor(0.9187)\n",
      "epoch: 16 loss: tensor(0.9203)\n",
      "epoch: 17 loss: tensor(0.9184)\n",
      "epoch: 18 loss: tensor(0.9200)\n",
      "epoch: 19 loss: tensor(0.9208)\n",
      "epoch: 20 loss: tensor(0.9199)\n",
      "epoch: 21 loss: tensor(0.9200)\n",
      "epoch: 22 loss: tensor(0.9195)\n",
      "epoch: 23 loss: tensor(0.9199)\n",
      "epoch: 24 loss: tensor(0.9190)\n",
      "epoch: 25 loss: tensor(0.9190)\n",
      "epoch: 26 loss: tensor(0.9190)\n",
      "epoch: 27 loss: tensor(0.9186)\n",
      "epoch: 28 loss: tensor(0.9190)\n",
      "epoch: 29 loss: tensor(0.9181)\n",
      "epoch: 30 loss: tensor(0.9183)\n",
      "epoch: 31 loss: tensor(0.9173)\n",
      "epoch: 32 loss: tensor(0.9179)\n",
      "epoch: 33 loss: tensor(0.9179)\n",
      "epoch: 34 loss: tensor(0.9187)\n",
      "epoch: 35 loss: tensor(0.9173)\n",
      "epoch: 36 loss: tensor(0.9174)\n",
      "epoch: 37 loss: tensor(0.9171)\n",
      "epoch: 38 loss: tensor(0.9170)\n",
      "epoch: 39 loss: tensor(0.9164)\n",
      "epoch: 40 loss: tensor(0.9166)\n",
      "epoch: 41 loss: tensor(0.9160)\n",
      "epoch: 42 loss: tensor(0.9162)\n",
      "epoch: 43 loss: tensor(0.9158)\n",
      "epoch: 44 loss: tensor(0.9155)\n",
      "epoch: 45 loss: tensor(0.9153)\n",
      "epoch: 46 loss: tensor(0.9159)\n",
      "epoch: 47 loss: tensor(0.9154)\n",
      "epoch: 48 loss: tensor(0.9155)\n",
      "epoch: 49 loss: tensor(0.9148)\n",
      "epoch: 50 loss: tensor(0.9151)\n",
      "epoch: 51 loss: tensor(0.9142)\n",
      "epoch: 52 loss: tensor(0.9149)\n",
      "epoch: 53 loss: tensor(0.9146)\n",
      "epoch: 54 loss: tensor(0.9149)\n",
      "epoch: 55 loss: tensor(0.9145)\n",
      "epoch: 56 loss: tensor(0.9144)\n",
      "epoch: 57 loss: tensor(0.9140)\n",
      "epoch: 58 loss: tensor(0.9139)\n",
      "epoch: 59 loss: tensor(0.9136)\n",
      "epoch: 60 loss: tensor(0.9138)\n",
      "epoch: 61 loss: tensor(0.9135)\n",
      "epoch: 62 loss: tensor(0.9138)\n",
      "epoch: 63 loss: tensor(0.9131)\n",
      "epoch: 64 loss: tensor(0.9134)\n",
      "epoch: 65 loss: tensor(0.9129)\n",
      "epoch: 66 loss: tensor(0.9132)\n",
      "epoch: 67 loss: tensor(0.9126)\n",
      "epoch: 68 loss: tensor(0.9128)\n",
      "epoch: 69 loss: tensor(0.9126)\n",
      "epoch: 70 loss: tensor(0.9126)\n",
      "epoch: 71 loss: tensor(0.9122)\n",
      "epoch: 72 loss: tensor(0.9123)\n",
      "epoch: 73 loss: tensor(0.9119)\n",
      "epoch: 74 loss: tensor(0.9124)\n",
      "epoch: 75 loss: tensor(0.9117)\n",
      "epoch: 76 loss: tensor(0.9120)\n",
      "epoch: 77 loss: tensor(0.9115)\n",
      "epoch: 78 loss: tensor(0.9114)\n",
      "epoch: 79 loss: tensor(0.9113)\n",
      "epoch: 80 loss: tensor(0.9114)\n",
      "epoch: 81 loss: tensor(0.9111)\n",
      "epoch: 82 loss: tensor(0.9111)\n",
      "epoch: 83 loss: tensor(0.9108)\n",
      "epoch: 84 loss: tensor(0.9108)\n",
      "epoch: 85 loss: tensor(0.9105)\n",
      "epoch: 86 loss: tensor(0.9106)\n",
      "epoch: 87 loss: tensor(0.9103)\n",
      "epoch: 88 loss: tensor(0.9105)\n",
      "epoch: 89 loss: tensor(0.9102)\n",
      "epoch: 90 loss: tensor(0.9100)\n",
      "epoch: 91 loss: tensor(0.9097)\n",
      "epoch: 92 loss: tensor(0.9096)\n",
      "epoch: 93 loss: tensor(0.9095)\n",
      "epoch: 94 loss: tensor(0.9094)\n",
      "epoch: 95 loss: tensor(0.9092)\n",
      "epoch: 96 loss: tensor(0.9092)\n",
      "epoch: 97 loss: tensor(0.9090)\n",
      "epoch: 98 loss: tensor(0.9090)\n",
      "epoch: 99 loss: tensor(0.9089)\n",
      "epoch: 100 loss: tensor(0.9090)\n",
      "epoch: 101 loss: tensor(0.9083)\n",
      "epoch: 102 loss: tensor(0.9082)\n",
      "epoch: 103 loss: tensor(0.9080)\n",
      "epoch: 104 loss: tensor(0.9081)\n",
      "epoch: 105 loss: tensor(0.9076)\n",
      "epoch: 106 loss: tensor(0.9076)\n",
      "epoch: 107 loss: tensor(0.9071)\n",
      "epoch: 108 loss: tensor(0.9069)\n",
      "epoch: 109 loss: tensor(0.9068)\n",
      "epoch: 110 loss: tensor(0.9066)\n",
      "epoch: 111 loss: tensor(0.9064)\n",
      "epoch: 112 loss: tensor(0.9058)\n",
      "epoch: 113 loss: tensor(0.9053)\n",
      "epoch: 114 loss: tensor(0.9056)\n",
      "epoch: 115 loss: tensor(0.9048)\n",
      "epoch: 116 loss: tensor(0.9045)\n",
      "epoch: 117 loss: tensor(0.9042)\n",
      "epoch: 118 loss: tensor(0.9042)\n",
      "epoch: 119 loss: tensor(0.9039)\n",
      "epoch: 120 loss: tensor(0.9036)\n",
      "epoch: 121 loss: tensor(0.9033)\n",
      "epoch: 122 loss: tensor(0.9027)\n",
      "epoch: 123 loss: tensor(0.9031)\n",
      "epoch: 124 loss: tensor(0.9020)\n",
      "epoch: 125 loss: tensor(0.9017)\n",
      "epoch: 126 loss: tensor(0.9011)\n",
      "epoch: 127 loss: tensor(0.9008)\n",
      "epoch: 128 loss: tensor(0.9004)\n",
      "epoch: 129 loss: tensor(0.9001)\n",
      "epoch: 130 loss: tensor(0.8993)\n",
      "epoch: 131 loss: tensor(0.8985)\n",
      "epoch: 132 loss: tensor(0.8980)\n",
      "epoch: 133 loss: tensor(0.8974)\n",
      "epoch: 134 loss: tensor(0.8967)\n",
      "epoch: 135 loss: tensor(0.8962)\n",
      "epoch: 136 loss: tensor(0.8956)\n",
      "epoch: 137 loss: tensor(0.8948)\n",
      "epoch: 138 loss: tensor(0.8946)\n",
      "epoch: 139 loss: tensor(0.8939)\n",
      "epoch: 140 loss: tensor(0.8933)\n",
      "epoch: 141 loss: tensor(0.8931)\n",
      "epoch: 142 loss: tensor(0.8924)\n",
      "epoch: 143 loss: tensor(0.8919)\n",
      "epoch: 144 loss: tensor(0.8911)\n",
      "epoch: 145 loss: tensor(0.8913)\n",
      "epoch: 146 loss: tensor(0.8909)\n",
      "epoch: 147 loss: tensor(0.8906)\n",
      "epoch: 148 loss: tensor(0.8895)\n",
      "epoch: 149 loss: tensor(0.8895)\n",
      "epoch: 150 loss: tensor(0.8882)\n",
      "epoch: 151 loss: tensor(0.8885)\n",
      "epoch: 152 loss: tensor(0.8875)\n",
      "epoch: 153 loss: tensor(0.8875)\n",
      "epoch: 154 loss: tensor(0.8865)\n",
      "epoch: 155 loss: tensor(0.8860)\n",
      "epoch: 156 loss: tensor(0.8853)\n",
      "epoch: 157 loss: tensor(0.8851)\n",
      "epoch: 158 loss: tensor(0.8846)\n",
      "epoch: 159 loss: tensor(0.8840)\n",
      "epoch: 160 loss: tensor(0.8837)\n",
      "epoch: 161 loss: tensor(0.8840)\n",
      "epoch: 162 loss: tensor(0.8825)\n",
      "epoch: 163 loss: tensor(0.8822)\n",
      "epoch: 164 loss: tensor(0.8825)\n",
      "epoch: 165 loss: tensor(0.8818)\n",
      "epoch: 166 loss: tensor(0.8835)\n",
      "epoch: 167 loss: tensor(0.8813)\n",
      "epoch: 168 loss: tensor(0.8809)\n",
      "epoch: 169 loss: tensor(0.8809)\n",
      "epoch: 170 loss: tensor(0.8803)\n",
      "epoch: 171 loss: tensor(0.8798)\n",
      "epoch: 172 loss: tensor(0.8795)\n",
      "epoch: 173 loss: tensor(0.8785)\n",
      "epoch: 174 loss: tensor(0.8784)\n",
      "epoch: 175 loss: tensor(0.8783)\n",
      "epoch: 176 loss: tensor(0.8781)\n",
      "epoch: 177 loss: tensor(0.8775)\n",
      "epoch: 178 loss: tensor(0.8771)\n",
      "epoch: 179 loss: tensor(0.8764)\n",
      "epoch: 180 loss: tensor(0.8763)\n",
      "epoch: 181 loss: tensor(0.8756)\n",
      "epoch: 182 loss: tensor(0.8755)\n",
      "epoch: 183 loss: tensor(0.8748)\n",
      "epoch: 184 loss: tensor(0.8747)\n",
      "epoch: 185 loss: tensor(0.8747)\n",
      "epoch: 186 loss: tensor(0.8743)\n",
      "epoch: 187 loss: tensor(0.8738)\n",
      "epoch: 188 loss: tensor(0.8735)\n",
      "epoch: 189 loss: tensor(0.8728)\n",
      "epoch: 190 loss: tensor(0.8731)\n",
      "epoch: 191 loss: tensor(0.8725)\n",
      "epoch: 192 loss: tensor(0.8724)\n",
      "epoch: 193 loss: tensor(0.8714)\n",
      "epoch: 194 loss: tensor(0.8718)\n",
      "epoch: 195 loss: tensor(0.8709)\n",
      "epoch: 196 loss: tensor(0.8709)\n",
      "epoch: 197 loss: tensor(0.8709)\n",
      "epoch: 198 loss: tensor(0.8708)\n",
      "epoch: 199 loss: tensor(0.8703)\n",
      "epoch: 200 loss: tensor(0.8699)\n"
     ]
    }
   ],
   "source": [
    "# Training the SAE\n",
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    normalizer = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        # define our input\n",
    "        # pytorch functions do not accept single 1d vectors\n",
    "        # so we will add a new dimension to our input at index 0\n",
    "        inputs = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = inputs.clone()\n",
    "        # we now test to only select rated movies (0 means no rating)\n",
    "        if (torch.sum(target.data > 0) > 0):\n",
    "            outputs = sae(inputs)\n",
    "            # we prevent algorithm from computing gradients at each step\n",
    "            # this saves time and space on computation\n",
    "            target.require_grad = False\n",
    "            outputs[target == 0] = 0 # restore unrated values\n",
    "            loss = criterion(outputs, target) # define our loss\n",
    "            # mean corrector = nb_movies / nb of rated movies\n",
    "            # we add + 1e-10 in order to prevent division by 0\n",
    "            # this corrector provides us with an average error only for rated movies\n",
    "            mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.data*mean_corrector)\n",
    "            normalizer += 1.\n",
    "            optimizer.step()\n",
    "    print('epoch: ' + str(epoch) + ' loss: '+ str(train_loss/normalizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: tensor(0.9404)\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "normalizer = 0.\n",
    "for id_user in range(nb_users):\n",
    "    inputs = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = Variable(test_set[id_user]).unsqueeze(0)\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        outputs = sae(inputs)\n",
    "        target.require_grad = False\n",
    "        outputs[target == 0] = 0\n",
    "        loss = criterion(outputs, target)\n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.data*mean_corrector)\n",
    "        normalizer += 1.\n",
    "print('test loss: '+str(test_loss/normalizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
