{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positive = Type 1 Error\n",
    "# False Negative = Type 2 Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you have a categorical dependent variable with 2 classes\n",
    "# Given some model, the resulting confusion matrix\n",
    "# is a 2 x 2 real matrix with the following info\n",
    "# y-true vs. y-predicted\n",
    "# (0, 0) entry = Number of correct predictions of class 0\n",
    "# (0, 1) entry = Number of incorrect predictions of class 0\n",
    "# (1, 0) entry = Number of incorrect predictions of class 1\n",
    "# (1, 1) entry = Number of correct predictions of class 1\n",
    "# (0, 1) entry = TYPE 1 ERROR\n",
    "# (1, 0) entry = TYPE 2 ERROR\n",
    "\n",
    "# From the confusion matrix we can calculate:\n",
    "# Accuracy Rate (AR) = #Correct / Total \n",
    "# Error Rate (ER) = #Errors / Total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Paradox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1:\n",
    "# Suppose we use a model and yield the following confusion matrix:\n",
    "# [[9700, 150], [50, 100]]\n",
    "# Note the large number of class 0 values \n",
    "# AR = 9800/10,000 = 0.98\n",
    "\n",
    "# With this knowledge we decide to exclusively make class 0\n",
    "# predictions\n",
    "# Scenario 2:\n",
    "# Our resulting confusion matrix becomes\n",
    "# [[9850, 0], [150, 0]]\n",
    "# AR = 9850/10,000 = 0.985\n",
    "\n",
    "# We have a higher accuracy rate!\n",
    "# However, the probability of Type 2 error has increased!\n",
    "# This yields an accuracy paradox\n",
    "# We use this as a warning: accuracy scores should be used\n",
    "# in a specific context\n",
    "# We need more robust ways of evaluating our classification models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAP Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative Accuracy Profile = CAP\n",
    "# Receiver Operating Characteristic = ROC\n",
    "# y-axis = Cumulative Number of Positive Outcomes\n",
    "# x-axis = Cumulative number of classifying parameter\n",
    "# We define Random CAP & a Perfect CAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAP Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Ratio (AR) = Area between model CAP & Random CAP \n",
    "#                      / Area between perfect CAP and Random CAP\n",
    "# AR is between 0 and 1\n",
    "# Higher values of AR indicate more accurate models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
