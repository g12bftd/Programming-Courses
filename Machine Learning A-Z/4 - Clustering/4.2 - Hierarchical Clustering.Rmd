---
title: "4.2 - Hierarchical Clustering"
author: "Glenn Bucagu"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~/Desktop/Projects/Machine Learning A-Z (Codes and Datasets)/Part 4 - Clustering/Section 25 - Hierarchical Clustering/R")
```


```{r importing dataset}
dataset <- read.csv("Mall_Customers.csv") 
dataset

# Using desired feature and output variable
X <- dataset[4:5]
X
```

```{r using dendrogram to optimize number of clusters}
dendrogram <- hclust(d = dist(X, method = "euclidean"),
                    method = "ward.D", #minimize within cluster variance
                    )
plot(dendrogram,
     main = paste("Dendrogram"),
     xlab = "Customers",
     ylab = "Euclidean Distances")

# By finding the largest vertical distance between jumps,
# and counting the number of clusters within that jump,
# we find that the optimal number of clusters is 5

```

```{r fitting the hierarchical clustering model}

hc <- hclust(d = dist(X, method = "euclidean"),
                    method = "ward.D", #minimize within cluster variance
                    )
y_hc <- cutree(tree = hc,
               k = 5)
y_hc


```


```{r visualizing model results}
#install.packages("cluster")
library(cluster)
clusplot(x = X,
         main = paste("Clusters of elements"),
         clus = y_hc,
         lines = 0,
         shade = TRUE,
         color = TRUE,
         labels = 2,
         plotchar = FALSE,
         xlab = "Annual Income ($)",
         ylab = "Spending Score")

```

